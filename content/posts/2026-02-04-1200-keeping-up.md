---
title: "Keeping Up"
slug: "keeping-up"
published: 2026-02-04
republished: []
last_build_hash: ""
last_build_lines: 0
---

I've been actively working with AI coding agents for months. I thought I was ahead of the curve. Then I went back and re-read something Steve Yegge wrote in March 2025, almost a year ago, and realized I'm still catching up.

## The Timeline Yegge Laid Out

In "[Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer)," Yegge described six overlapping waves of AI-assisted programming: traditional coding, code completions, chat-based coding, coding agents, agent clusters, and agent fleets. He estimated agent clusters would arrive by Q3 2025 and agent fleets by early 2026.

At the time, I read it and thought it was interesting but maybe a bit aggressive. I was just starting to experiment with agents myself. I'd already rejected the autocomplete phase because it disrupted my thinking flow. I was exploring chat-based workflows and feeling pretty forward-thinking about it.

What I didn't do was actually internalize what he was saying.

## Where I Thought I Was vs Where I Actually Was

When I started Ikigai, I believed I was thinking at the agent clusters level. Multiple agents working in parallel, isolated workspaces, orchestration layers. That felt like frontier thinking.

Looking back at my own recent posts tells a different story. "Thinking in Silos" from late January was me discovering that work needs to be decomposed for parallel execution. "Disposable Clones" from early February was me working out the isolation mechanics for running multiple agents. "Collaborators and Workers" just yesterday was me realizing that orchestration splits into collaborative planning and mechanical distribution.

These are all real insights that I needed to work through. But Yegge had already mapped this territory a year earlier. The concept of agent clusters (developers managing many parallel agents), supervisor agents that keep workers unstuck, work queues that agents consume autonomously, all of it was there in his post.

I wasn't pioneering. I was slowly rediscovering.

## What Woke Me Up

Several things helped me see this more clearly.

The Ralph Loop emerged as a pattern where you run an AI agent in a loop until work is complete. Progress lives in files and git history, not in the agent's context window. When context fills up, you get a fresh agent that picks up where the last one left off. Geoffrey Huntley formalized this as a technique, and it became an actual movement. The ralph tool I've been using follows this pattern exactly.

Clawdbot (now called OpenClaw) showed up in January as a self-hosted personal AI assistant that connects to services you already use and actually does things, not just chats. It went viral. Peter Steinberger built it, and suddenly everyone was talking about AI agents that operate autonomously across your digital life.

Then Yegge himself released [Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04), his agent orchestrator. It's exactly what he predicted a year earlier: a system for running 20 to 30 agents at once, with patrol workers that keep things moving, merge queues that handle coordination, and workflows expressed as chains of work items. He called it "Kubernetes for agents" and built the thing.

All of this happened while I was carefully working out basic parallel execution patterns.

## Why This Is Hard to Internalize

Even when you're actively working in this space, it's remarkably difficult to absorb what's happening. I read Yegge's posts when they came out. I was using ralph. I saw the conversation around Clawdbot. None of it fully registered.

Part of it is that predictions feel speculative until you've personally experienced the constraints they address. I needed to run into merge conflicts across parallel agents before I appreciated the merge queue problem. I needed to watch agents get stuck mid-task before I understood why patrol workers matter. The knowledge wasn't useful until I had the problems.

Part of it is that exponential change doesn't feel exponential from the inside. Each step seems incremental. Going from one agent to two felt like a small optimization. Going from two to five felt like more of the same. But somewhere in that progression the nature of the work changes, and you're doing something qualitatively different.

And part of it is just cognitive resistance. When someone says agent fleets will manage hundreds of workers with AI supervisors, the natural response is "that seems like too much too fast." It's easier to file it away as interesting speculation than to actually plan around it.

## The Exercise

Here's what I'd suggest: Think about where your understanding of AI coding was a year ago. Think about where it is now. Then read Yegge's "[Revenge of the Junior Developer](https://sourcegraph.com/blog/revenge-of-the-junior-developer)" from March 2025. Then read his "[Welcome to Gas Town](https://steve-yegge.medium.com/welcome-to-gas-town-4f25ee16dd04)" from January 2026.

Notice how much of what seemed speculative a year ago is now operational. Notice how much of what seems speculative today probably isn't.

We're all behind. The question is how much effort we put into catching up.

---

*Co-authored by Mike Greenly and Claude Code*
